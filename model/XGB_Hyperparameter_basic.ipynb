{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import module, function and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import validation_curve\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier # rf분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(y_pred, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    \n",
    "    pre = precision_score(y_true = labels, y_pred = y_pred, average=None)\n",
    "    rec = recall_score(y_true = labels, y_pred = y_pred, average=None)\n",
    "    f1_score = 8/(sum(1/pre) + sum(1/rec))\n",
    "\n",
    "    return 'f1', f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(X_val, y_val, model,mapping):\n",
    "    \"\"\"\n",
    "    Model evaluation function for multiclass classification problem\n",
    "    1) F-1 score, Precision, Recall\n",
    "    2) ROC curve, PR curve는 추후에 생각\n",
    "    \"\"\"\n",
    " \n",
    "    #### predict the value\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    #print('-'*50)\n",
    "    #print('2. F1-score')\n",
    "    \n",
    "    # inverse pre/ rec\n",
    "    pre = precision_score(y_true = y_val, y_pred = y_pred, average=None)\n",
    "    rec = recall_score(y_true = y_val, y_pred = y_pred, average=None)\n",
    "\n",
    "    # f1 measure\n",
    "    f1_score = 8/(sum(1/pre) + sum(1/rec))\n",
    "    \n",
    "    # view - precision recall\n",
    "    table = pd.DataFrame([])\n",
    "\n",
    "    for i,k in enumerate(mapping.keys()):\n",
    "        table[k] = [pre[i],rec[i]]\n",
    "    table.index = ['precision','recall']\n",
    "    # print(table)\n",
    "    \n",
    "    # view - f1\n",
    "    #print('F1_score %.3f'%f1_score)\n",
    "    #print('='*50)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load data set\n",
    "## train\n",
    "X_train1 = pd.read_csv('temp_data/X_train_stat.csv')\n",
    "X_train1.columns = ['new_id'] + [x +'_stat' for x in X_train1.columns[1:]]\n",
    "X_train2 = pd.read_csv('temp_data/X_train_easy_time.csv').drop('new_id',axis=1)\n",
    "X_train2.columns = [x +'_basic_time' for x in X_train2.columns]\n",
    "X_train3 = pd.read_csv('temp_data/X_train_게임활동_time.csv').drop('new_id',axis=1)\n",
    "X_train3.columns = [x +'_time_series' for x in X_train3.columns]\n",
    "X_train6 = pd.read_csv('temp_data/X_train_ratio.csv').drop('new_id',axis=1)\n",
    "X_train6.fillna(0.0)\n",
    "X_train9 = pd.read_csv('temp_data/X_train_act_comb_1.csv').drop('new_id',axis=1)\n",
    "X_train10 = pd.read_csv('temp_data/train_playpattern_mean_encoding.csv').drop('new_id',axis=1)\n",
    "\n",
    "## test\n",
    "X_test1 = pd.read_csv('temp_data/X_test_stat.csv')\n",
    "X_test1.columns = X_train1.columns\n",
    "X_test2 = pd.read_csv('temp_data/X_test_easy_time.csv').drop('new_id',axis=1)\n",
    "X_test2.columns = X_train2.columns\n",
    "X_test3 = pd.read_csv('temp_data/X_test_게임활동_time.csv').drop('new_id',axis=1)\n",
    "X_test3.columns = X_train3.columns\n",
    "X_test6 = pd.read_csv('temp_data/X_test_ratio.csv').drop('new_id',axis=1)\n",
    "X_test6.fillna(0.0)\n",
    "X_test9 = pd.read_csv('temp_data/X_test_act_comb_1.csv').drop('new_id',axis=1)\n",
    "X_test10 = pd.read_csv('temp_data/test_playpattern_mean_encoding.csv').drop('new_id',axis=1)\n",
    "\n",
    "\n",
    "## guild and trade and party\n",
    "X_train4 = pd.read_csv('temp_data/temp_guild_train.csv').drop('new_id',axis=1)\n",
    "X_train5 = pd.read_csv('temp_data/temp_trade_train.csv').drop('new_id',axis=1)\n",
    "X_test4 = pd.read_csv('temp_data/temp_guild_test.csv').drop('new_id',axis=1)\n",
    "X_test5 = pd.read_csv('temp_data/temp_trade_test.csv').drop('new_id',axis=1)\n",
    "X_train7 = pd.read_csv('temp_data/X_train_party.csv').drop('new_id',axis=1)\n",
    "X_test7 = pd.read_csv('temp_data/X_test_party.csv').drop('new_id',axis=1)\n",
    "X_train8 = pd.read_csv('temp_data/training_trade.csv').drop('new_id',axis=1)\n",
    "X_test8 = pd.read_csv('temp_data/test_trade.csv').drop('new_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat((X_train1,X_train2,X_train3,X_train4,X_train5, X_train6,X_train7,X_train8,X_train9,X_train10),axis=1).drop('new_id',axis=1)\n",
    "X_test = pd.concat((X_test1,X_test2,X_test3,X_test4,X_test5,X_test6,X_test7,X_test8,X_test9,X_test10),axis=1).drop('new_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load class\n",
    "train_label = pd.read_csv('temp_data/train_label_lite.csv')\n",
    "# hasher = pd.read_csv('test_id.csv')\n",
    "label_map = {'retained':0,'2month':1,'month':2,'week':3}\n",
    "y_train = pd.Series([label_map[l] for l in train_label.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inf가 존재\n",
    "X_train = X_train.drop('cnt_use_buffitem_by_game_combat_time',axis = 1)\n",
    "X_test = X_test.drop('cnt_use_buffitem_by_game_combat_time',axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1161)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RF 모델\n",
    "model = RandomForestClassifier(criterion='gini','max_features': 243, 'min_samples_leaf': 4,n_estimators=300,random_state= 7, n_jobs=-1)\n",
    "X_train_rf = X_train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Class dist.: [22500 22500 22500 22500], F1: 0.715\n",
      "Fold: 2, Class dist.: [22500 22500 22500 22500], F1: 0.718\n",
      "Fold: 3, Class dist.: [22500 22500 22500 22500], F1: 0.725\n",
      "Fold: 4, Class dist.: [22500 22500 22500 22500], F1: 0.726\n",
      "Fold: 5, Class dist.: [22500 22500 22500 22500], F1: 0.714\n",
      "Fold: 6, Class dist.: [22500 22500 22500 22500], F1: 0.718\n",
      "Fold: 7, Class dist.: [22500 22500 22500 22500], F1: 0.715\n",
      "Fold: 8, Class dist.: [22500 22500 22500 22500], F1: 0.712\n",
      "Fold: 9, Class dist.: [22500 22500 22500 22500], F1: 0.707\n",
      "Fold: 10, Class dist.: [22500 22500 22500 22500], F1: 0.711\n",
      "\n",
      "CV F1: 0.716 +/- 0.006\n"
     ]
    }
   ],
   "source": [
    "#### cross validation\n",
    "kfold = StratifiedKFold(n_splits = 10 ,random_state = 7).split(X_train_rf, y_train)\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    model.fit(X_train_rf.iloc[train,:], y_train[train])\n",
    "    score = f1(X_train_rf.iloc[test,:], y_train[test], model,label_map)\n",
    "    scores.append(score)\n",
    "    print('Fold: %s, Class dist.: %s, F1: %.3f' % (k+1,np.bincount(y_train[train]), score))\n",
    "    \n",
    "print('\\nCV F1: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### feature selection by RF\n",
    "importances = model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_],axis=0)\n",
    "indices = np.argsort(importances) # ascending\n",
    "\n",
    "#### feature ranking\n",
    "feature_ranking = [(indices[f],importances[indices[f]]) for f in range(X_train.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### state of art feature.... 300개 정도... importance ratio 조정!!!\n",
    "NUM_OF_FEATURES = len([(i,f) for i, f in feature_ranking if f > 0.0001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_OF_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = pd.DataFrame({'importance': model.feature_importances_, 'feature': X_train.columns}).sort_values(by=['importance'], ascending=[False])[:NUM_OF_FEATURES]['feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 693)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FEATURE SELECTION\n",
    "X_train = X_train[col]\n",
    "X_test = X_test[col]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperopt Xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_classifier(params): # hyperopt의 objective function은 params를 input으로 받는다.\n",
    "    \n",
    "    global obj_call_count, cur_best_score, cur_best_std, X_train, y_train # 우리가 input할 데이터는 global변수화!\n",
    "    \n",
    "    obj_call_count += 1\n",
    "    print('\\nXgboost objective call #{} cur_best_score={:7.5f} cur_best_std={:7.5f}'.format(obj_call_count,cur_best_score,cur_best_std) )\n",
    "    \n",
    "    #### sampling parameters from the hyperparameter params\n",
    "    xgb_params = sample(params)\n",
    "    model = XGBClassifier(**xgb_params)\n",
    "    \n",
    "    #### 5 fold cross validation for rf\n",
    "    kfold = StratifiedKFold(n_splits = 5 ,random_state = 7,shuffle=True).split(X_train, y_train)\n",
    "    scores = []\n",
    "    predict_set = []\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "        model.fit(X_train.iloc[train,:], y_train[train],eval_metric = F1)\n",
    "        scores.append(score)\n",
    "        print('Fold: %s, Class dist.: %s, F1: %.3f' % (k+1,np.bincount(y_train[train]), score))\n",
    "        \n",
    "        ### predict\n",
    "        y_pred = model.predict(X_train.iloc[test,:])\n",
    "        predict_set += [(x,inv_map[y_pred[i]]) for i,x in enumerate(test)]\n",
    "        \n",
    "    f1_mean = np.mean(scores)\n",
    "    f1_std = np.std(scores)\n",
    "    \n",
    "    print(xgb_params)\n",
    "    print('5-fold of Xgboost F1: %.3f +/- %.3f' % (f1_mean, f1_std))\n",
    "    \n",
    "    if f1_mean > cur_best_score:\n",
    "        cur_best_score = f1_mean\n",
    "        cur_best_std = f1_std\n",
    "        \n",
    "    #### minimize metric\n",
    "    loss = 1 - f1_mean\n",
    "    loss_var = 1 - np.var(scores)\n",
    "    \n",
    "    return {'loss': loss , 'loss_variance': loss_var ,'status':STATUS_OK ,'attachments':{'pred_cv':predict_set}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_call_count = 0\n",
    "cur_best_score = 0\n",
    "cur_best_std = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'n_estimators': 369,\n",
    "    'learning_rate': 0.1\n",
    "    'min_child_weight': hp.choice('min_child_weight',range(3,10))\n",
    "    'max_depth':12\n",
    "\n",
    "    'reg_alpha': hp.quniform('reg_alpha', 0, 0.1, 0.001)\n",
    "    'reg_lambda': hp.quniform('reg_lambda', 0, 0.1, 0.001)\n",
    "    'subsample': hp.quniform('subsample', 0.6, 1, 0.03)\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 1, 0.03)\n",
    "    \n",
    "    'num_class':4\n",
    "    'objective': 'multi:softmax'\n",
    "    'seed': 7\n",
    "    \n",
    "    'n_gpus' : -1\n",
    "    'tree_method' : 'gpu_hist'\n",
    "    'silent' : 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1 :  tuning n_estimators with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 새로운 피쳐를 많이 넣으면 다시 돌려주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### xgb\n",
    "grid_result = []\n",
    "param = {}\n",
    "#### XGB parameters\n",
    "## General Parameters\n",
    "param['n_gpus'] = -1\n",
    "param['tree_method'] = 'gpu_hist'\n",
    "param['silent'] = 0\n",
    "\n",
    "## Booster Parameters\n",
    "param['n_estimators'] = 369 #요기...\n",
    "param['learning_rate'] = 0.1\n",
    "param['min_child_weight'] = 4\n",
    "param['max_depth'] = 10\n",
    "param['gamma'] = 0\n",
    "param['reg_alpha'] = 0\n",
    "param['reg_lambda'] = 0\n",
    "param['subsample'] = 0.95\n",
    "param['colsample_bytree'] = 0.75\n",
    "param['scale_pos_weight'] = 1\n",
    "\n",
    "## Learning task parameters\n",
    "param['num_class'] = 4\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['seed'] = 7\n",
    "\n",
    "cv_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Find the n_estimators\n",
      "Optimal n_estimators : 227\n"
     ]
    }
   ],
   "source": [
    " #### step 1 : tuning n_estimators with cross validation\n",
    "print(\"===============================================\")\n",
    "print(\"Find the n_estimators\")\n",
    "xgtrain = xgb.DMatrix(X_train.values, label= y_train.values.reshape(-1,1))\n",
    "cvresult = xgb.cv(param, xgtrain, num_boost_round = param['n_estimators'], nfold = 5, metrics = \"mlogloss\", early_stopping_rounds = 50)\n",
    "print(\"Optimal n_estimators : %d\"%cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: Tune max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### xgb\n",
    "grid_result = []\n",
    "param = {}\n",
    "#### XGB parameters\n",
    "## General Parameters\n",
    "param['n_gpus'] = -1\n",
    "param['tree_method'] = 'gpu_hist'\n",
    "param['silent'] = 0\n",
    "\n",
    "## Booster Parameters\n",
    "param['n_estimators'] = 369 #요기...\n",
    "param['learning_rate'] = 0.1\n",
    "param['min_child_weight'] = 6\n",
    "param['max_depth'] = 12\n",
    "param['gamma'] = 0\n",
    "param['reg_alpha'] = 0\n",
    "param['reg_lambda'] = 0\n",
    "param['subsample'] = 0.95\n",
    "param['colsample_bytree'] = 0.75\n",
    "param['scale_pos_weight'] = 1\n",
    "\n",
    "## Learning task parameters\n",
    "param['num_class'] = 4\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['seed'] = 7\n",
    "\n",
    "cv_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_gpus': -1,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'silent': 0,\n",
       " 'n_estimators': 369,\n",
       " 'learning_rate': 0.1,\n",
       " 'min_child_weight': 4,\n",
       " 'max_depth': 10,\n",
       " 'gamma': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 0,\n",
       " 'subsample': 0.95,\n",
       " 'colsample_bytree': 0.75,\n",
       " 'scale_pos_weight': 1,\n",
       " 'num_class': 4,\n",
       " 'objective': 'multi:softmax',\n",
       " 'seed': 7}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.75, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=4, missing=None, n_estimators=369,\n",
       "       n_gpus=-1, n_jobs=1, nthread=None, num_class=4,\n",
       "       objective='multi:softmax', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=0, scale_pos_weight=1, seed=7, silent=0, subsample=0.95,\n",
       "       tree_method='gpu_hist'),\n",
       "       fit_params=None, iid=False, n_jobs=1,\n",
       "       param_grid={'subsample': [0.85, 0.88, 0.9, 0.93, 0.95, 0.98], 'colsample_bytree': [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.98]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_macro', verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### step 4: Tune subsample and colsample_bytree\n",
    "print()\n",
    "print(\"===============================================\")\n",
    "param_tune_sub = {}\n",
    "param_tune_sub['subsample'] = [0.85,0.88,0.9,0.93,0.95,0.98]\n",
    "param_tune_sub['colsample_bytree'] = [0.7,0.75,0.8,0.85,0.9,0.95,0.98]\n",
    "\n",
    "grid = GridSearchCV(estimator = XGBClassifier(**param),param_grid = param_tune_sub, scoring = 'f1_macro',iid=False ,n_jobs = 1, cv = cv_folds)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the subsample and colsample_bytree\n",
      "Grid Scores\n",
      "[mean: 0.73660, std: 0.00431, params: {'colsample_bytree': 0.7, 'subsample': 0.85},\n",
      " mean: 0.73731, std: 0.00460, params: {'colsample_bytree': 0.7, 'subsample': 0.88},\n",
      " mean: 0.73645, std: 0.00507, params: {'colsample_bytree': 0.7, 'subsample': 0.9},\n",
      " mean: 0.73778, std: 0.00332, params: {'colsample_bytree': 0.7, 'subsample': 0.93},\n",
      " mean: 0.73750, std: 0.00445, params: {'colsample_bytree': 0.7, 'subsample': 0.95},\n",
      " mean: 0.73715, std: 0.00490, params: {'colsample_bytree': 0.7, 'subsample': 0.98},\n",
      " mean: 0.73704, std: 0.00491, params: {'colsample_bytree': 0.75, 'subsample': 0.85},\n",
      " mean: 0.73765, std: 0.00420, params: {'colsample_bytree': 0.75, 'subsample': 0.88},\n",
      " mean: 0.73817, std: 0.00499, params: {'colsample_bytree': 0.75, 'subsample': 0.9},\n",
      " mean: 0.73712, std: 0.00450, params: {'colsample_bytree': 0.75, 'subsample': 0.93},\n",
      " mean: 0.73813, std: 0.00392, params: {'colsample_bytree': 0.75, 'subsample': 0.95},\n",
      " mean: 0.73716, std: 0.00425, params: {'colsample_bytree': 0.75, 'subsample': 0.98},\n",
      " mean: 0.73779, std: 0.00367, params: {'colsample_bytree': 0.8, 'subsample': 0.85},\n",
      " mean: 0.73735, std: 0.00353, params: {'colsample_bytree': 0.8, 'subsample': 0.88},\n",
      " mean: 0.73733, std: 0.00437, params: {'colsample_bytree': 0.8, 'subsample': 0.9},\n",
      " mean: 0.73790, std: 0.00413, params: {'colsample_bytree': 0.8, 'subsample': 0.93},\n",
      " mean: 0.73761, std: 0.00379, params: {'colsample_bytree': 0.8, 'subsample': 0.95},\n",
      " mean: 0.73808, std: 0.00441, params: {'colsample_bytree': 0.8, 'subsample': 0.98},\n",
      " mean: 0.73699, std: 0.00460, params: {'colsample_bytree': 0.85, 'subsample': 0.85},\n",
      " mean: 0.73697, std: 0.00397, params: {'colsample_bytree': 0.85, 'subsample': 0.88},\n",
      " mean: 0.73644, std: 0.00478, params: {'colsample_bytree': 0.85, 'subsample': 0.9},\n",
      " mean: 0.73665, std: 0.00405, params: {'colsample_bytree': 0.85, 'subsample': 0.93},\n",
      " mean: 0.73689, std: 0.00484, params: {'colsample_bytree': 0.85, 'subsample': 0.95},\n",
      " mean: 0.73739, std: 0.00469, params: {'colsample_bytree': 0.85, 'subsample': 0.98},\n",
      " mean: 0.73736, std: 0.00435, params: {'colsample_bytree': 0.9, 'subsample': 0.85},\n",
      " mean: 0.73657, std: 0.00455, params: {'colsample_bytree': 0.9, 'subsample': 0.88},\n",
      " mean: 0.73748, std: 0.00480, params: {'colsample_bytree': 0.9, 'subsample': 0.9},\n",
      " mean: 0.73728, std: 0.00509, params: {'colsample_bytree': 0.9, 'subsample': 0.93},\n",
      " mean: 0.73751, std: 0.00433, params: {'colsample_bytree': 0.9, 'subsample': 0.95},\n",
      " mean: 0.73670, std: 0.00457, params: {'colsample_bytree': 0.9, 'subsample': 0.98},\n",
      " mean: 0.73695, std: 0.00393, params: {'colsample_bytree': 0.95, 'subsample': 0.85},\n",
      " mean: 0.73734, std: 0.00404, params: {'colsample_bytree': 0.95, 'subsample': 0.88},\n",
      " mean: 0.73730, std: 0.00380, params: {'colsample_bytree': 0.95, 'subsample': 0.9},\n",
      " mean: 0.73698, std: 0.00452, params: {'colsample_bytree': 0.95, 'subsample': 0.93},\n",
      " mean: 0.73718, std: 0.00333, params: {'colsample_bytree': 0.95, 'subsample': 0.95},\n",
      " mean: 0.73660, std: 0.00351, params: {'colsample_bytree': 0.95, 'subsample': 0.98},\n",
      " mean: 0.73704, std: 0.00461, params: {'colsample_bytree': 0.98, 'subsample': 0.85},\n",
      " mean: 0.73735, std: 0.00411, params: {'colsample_bytree': 0.98, 'subsample': 0.88},\n",
      " mean: 0.73770, std: 0.00398, params: {'colsample_bytree': 0.98, 'subsample': 0.9},\n",
      " mean: 0.73784, std: 0.00411, params: {'colsample_bytree': 0.98, 'subsample': 0.93},\n",
      " mean: 0.73644, std: 0.00441, params: {'colsample_bytree': 0.98, 'subsample': 0.95},\n",
      " mean: 0.73722, std: 0.00450, params: {'colsample_bytree': 0.98, 'subsample': 0.98}]\n",
      "Best parameter - subsample and colsample_bytree\n",
      "{'colsample_bytree': 0.75, 'subsample': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Find the subsample and colsample_bytree\")\n",
    "print(\"Grid Scores\")\n",
    "pprint(grid.grid_scores_)\n",
    "grid_result.append(grid.grid_scores_)\n",
    "print(\"Best parameter - subsample and colsample_bytree\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "## update\n",
    "param.update(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 5: Regularization alpha = L1, lambda = L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.75, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=4, missing=None, n_estimators=369,\n",
       "       n_gpus=-1, n_jobs=1, nthread=None, num_class=4,\n",
       "       objective='multi:softmax', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=0, scale_pos_weight=1, seed=7, silent=0, subsample=0.9,\n",
       "       tree_method='gpu_hist'),\n",
       "       fit_params=None, iid=False, n_jobs=1,\n",
       "       param_grid={'reg_alpha': [0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1], 'reg_lambda': [0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_macro', verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### step 5: Regularization alpha = L1, lambda = L2\n",
    "print()\n",
    "print(\"===============================================\")\n",
    "param_tune_reg = {}\n",
    "param_tune_reg['reg_alpha'] = [0.001,0.003,0.006,0.01,0.03,0.06,0.1]\n",
    "param_tune_reg['reg_lambda'] = [0.001,0.003,0.006,0.01,0.03,0.06,0.1]\n",
    "\n",
    "grid = GridSearchCV(estimator = XGBClassifier(**param),param_grid = param_tune_reg, scoring = 'f1_macro',iid=False ,n_jobs = 1, cv = cv_folds)\n",
    "grid.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the Regularization alpha = L1, lambda = L2\n",
      "Grid Scores\n",
      "[mean: 0.73718, std: 0.00459, params: {'reg_alpha': 0.001, 'reg_lambda': 0.001},\n",
      " mean: 0.73765, std: 0.00470, params: {'reg_alpha': 0.001, 'reg_lambda': 0.003},\n",
      " mean: 0.73747, std: 0.00404, params: {'reg_alpha': 0.001, 'reg_lambda': 0.006},\n",
      " mean: 0.73854, std: 0.00530, params: {'reg_alpha': 0.001, 'reg_lambda': 0.01},\n",
      " mean: 0.73724, std: 0.00451, params: {'reg_alpha': 0.001, 'reg_lambda': 0.03},\n",
      " mean: 0.73842, std: 0.00415, params: {'reg_alpha': 0.001, 'reg_lambda': 0.06},\n",
      " mean: 0.73826, std: 0.00473, params: {'reg_alpha': 0.001, 'reg_lambda': 0.1},\n",
      " mean: 0.73814, std: 0.00449, params: {'reg_alpha': 0.003, 'reg_lambda': 0.001},\n",
      " mean: 0.73810, std: 0.00458, params: {'reg_alpha': 0.003, 'reg_lambda': 0.003},\n",
      " mean: 0.73762, std: 0.00490, params: {'reg_alpha': 0.003, 'reg_lambda': 0.006},\n",
      " mean: 0.73756, std: 0.00415, params: {'reg_alpha': 0.003, 'reg_lambda': 0.01},\n",
      " mean: 0.73779, std: 0.00445, params: {'reg_alpha': 0.003, 'reg_lambda': 0.03},\n",
      " mean: 0.73824, std: 0.00453, params: {'reg_alpha': 0.003, 'reg_lambda': 0.06},\n",
      " mean: 0.73779, std: 0.00396, params: {'reg_alpha': 0.003, 'reg_lambda': 0.1},\n",
      " mean: 0.73826, std: 0.00480, params: {'reg_alpha': 0.006, 'reg_lambda': 0.001},\n",
      " mean: 0.73746, std: 0.00500, params: {'reg_alpha': 0.006, 'reg_lambda': 0.003},\n",
      " mean: 0.73756, std: 0.00406, params: {'reg_alpha': 0.006, 'reg_lambda': 0.006},\n",
      " mean: 0.73799, std: 0.00425, params: {'reg_alpha': 0.006, 'reg_lambda': 0.01},\n",
      " mean: 0.73768, std: 0.00431, params: {'reg_alpha': 0.006, 'reg_lambda': 0.03},\n",
      " mean: 0.73804, std: 0.00459, params: {'reg_alpha': 0.006, 'reg_lambda': 0.06},\n",
      " mean: 0.73815, std: 0.00408, params: {'reg_alpha': 0.006, 'reg_lambda': 0.1},\n",
      " mean: 0.73717, std: 0.00490, params: {'reg_alpha': 0.01, 'reg_lambda': 0.001},\n",
      " mean: 0.73782, std: 0.00415, params: {'reg_alpha': 0.01, 'reg_lambda': 0.003},\n",
      " mean: 0.73841, std: 0.00432, params: {'reg_alpha': 0.01, 'reg_lambda': 0.006},\n",
      " mean: 0.73754, std: 0.00505, params: {'reg_alpha': 0.01, 'reg_lambda': 0.01},\n",
      " mean: 0.73770, std: 0.00414, params: {'reg_alpha': 0.01, 'reg_lambda': 0.03},\n",
      " mean: 0.73812, std: 0.00421, params: {'reg_alpha': 0.01, 'reg_lambda': 0.06},\n",
      " mean: 0.73759, std: 0.00400, params: {'reg_alpha': 0.01, 'reg_lambda': 0.1},\n",
      " mean: 0.73711, std: 0.00416, params: {'reg_alpha': 0.03, 'reg_lambda': 0.001},\n",
      " mean: 0.73804, std: 0.00434, params: {'reg_alpha': 0.03, 'reg_lambda': 0.003},\n",
      " mean: 0.73784, std: 0.00434, params: {'reg_alpha': 0.03, 'reg_lambda': 0.006},\n",
      " mean: 0.73801, std: 0.00412, params: {'reg_alpha': 0.03, 'reg_lambda': 0.01},\n",
      " mean: 0.73764, std: 0.00500, params: {'reg_alpha': 0.03, 'reg_lambda': 0.03},\n",
      " mean: 0.73738, std: 0.00471, params: {'reg_alpha': 0.03, 'reg_lambda': 0.06},\n",
      " mean: 0.73798, std: 0.00417, params: {'reg_alpha': 0.03, 'reg_lambda': 0.1},\n",
      " mean: 0.73862, std: 0.00468, params: {'reg_alpha': 0.06, 'reg_lambda': 0.001},\n",
      " mean: 0.73653, std: 0.00426, params: {'reg_alpha': 0.06, 'reg_lambda': 0.003},\n",
      " mean: 0.73786, std: 0.00410, params: {'reg_alpha': 0.06, 'reg_lambda': 0.006},\n",
      " mean: 0.73779, std: 0.00531, params: {'reg_alpha': 0.06, 'reg_lambda': 0.01},\n",
      " mean: 0.73797, std: 0.00509, params: {'reg_alpha': 0.06, 'reg_lambda': 0.03},\n",
      " mean: 0.73808, std: 0.00466, params: {'reg_alpha': 0.06, 'reg_lambda': 0.06},\n",
      " mean: 0.73725, std: 0.00421, params: {'reg_alpha': 0.06, 'reg_lambda': 0.1},\n",
      " mean: 0.73675, std: 0.00447, params: {'reg_alpha': 0.1, 'reg_lambda': 0.001},\n",
      " mean: 0.73778, std: 0.00454, params: {'reg_alpha': 0.1, 'reg_lambda': 0.003},\n",
      " mean: 0.73778, std: 0.00455, params: {'reg_alpha': 0.1, 'reg_lambda': 0.006},\n",
      " mean: 0.73759, std: 0.00430, params: {'reg_alpha': 0.1, 'reg_lambda': 0.01},\n",
      " mean: 0.73864, std: 0.00422, params: {'reg_alpha': 0.1, 'reg_lambda': 0.03},\n",
      " mean: 0.73778, std: 0.00467, params: {'reg_alpha': 0.1, 'reg_lambda': 0.06},\n",
      " mean: 0.73778, std: 0.00467, params: {'reg_alpha': 0.1, 'reg_lambda': 0.1}]\n",
      "Best parameter - Regularization alpha = L1, lambda = L2\n",
      "{'reg_alpha': 0.1, 'reg_lambda': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Find the Regularization alpha = L1, lambda = L2\")\n",
    "print(\"Grid Scores\")\n",
    "pprint(grid.grid_scores_)\n",
    "grid_result.append(grid.grid_scores_)\n",
    "print(\"Best parameter - Regularization alpha = L1, lambda = L2\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "## update\n",
    "param.update(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_gpus': -1,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'silent': 0,\n",
       " 'n_estimators': 369,\n",
       " 'learning_rate': 0.1,\n",
       " 'min_child_weight': 4,\n",
       " 'max_depth': 10,\n",
       " 'gamma': 0,\n",
       " 'reg_alpha': 0.1,\n",
       " 'reg_lambda': 0.03,\n",
       " 'subsample': 0.9,\n",
       " 'colsample_bytree': 0.75,\n",
       " 'scale_pos_weight': 1,\n",
       " 'num_class': 4,\n",
       " 'objective': 'multi:softmax',\n",
       " 'seed': 7}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step : Tune the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### xgb\n",
    "grid_result = []\n",
    "param = {}\n",
    "#### XGB parameters\n",
    "## General Parameters\n",
    "param['n_gpus'] = -1\n",
    "param['tree_method'] = 'gpu_hist'\n",
    "param['silent'] = 0\n",
    "\n",
    "## Booster Parameters\n",
    "param['n_estimators'] = 227 #요기...\n",
    "param['learning_rate'] = 0.1\n",
    "param['min_child_weight'] = 4\n",
    "param['max_depth'] = 10\n",
    "param['gamma'] = 0\n",
    "param['reg_alpha'] = 0.1\n",
    "param['reg_lambda'] = 0.03\n",
    "param['subsample'] = 0.9\n",
    "param['colsample_bytree'] = 0.75\n",
    "param['scale_pos_weight'] = 1\n",
    "\n",
    "## Learning task parameters\n",
    "param['num_class'] = 4\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['seed'] = 7\n",
    "\n",
    "cv_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-mlogloss-mean</th>\n",
       "      <th>train-mlogloss-std</th>\n",
       "      <th>test-mlogloss-mean</th>\n",
       "      <th>test-mlogloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.376122</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>1.376718</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.366150</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>1.367342</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.356351</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1.358133</td>\n",
       "      <td>0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.346763</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>1.349125</td>\n",
       "      <td>0.000184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.337293</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>1.340250</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.327997</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.331541</td>\n",
       "      <td>0.000231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.318822</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>1.322930</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.309828</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>1.314488</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.300931</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>1.306182</td>\n",
       "      <td>0.000338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.292124</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>1.297967</td>\n",
       "      <td>0.000347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.283534</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>1.289945</td>\n",
       "      <td>0.000354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.275059</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>1.282037</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.266687</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.274229</td>\n",
       "      <td>0.000401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.258490</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>1.266578</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.250427</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>1.259072</td>\n",
       "      <td>0.000499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.242452</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>1.251667</td>\n",
       "      <td>0.000552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.234642</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>1.244387</td>\n",
       "      <td>0.000603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.226906</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>1.237205</td>\n",
       "      <td>0.000636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.219303</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>1.230154</td>\n",
       "      <td>0.000685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.211777</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>1.223186</td>\n",
       "      <td>0.000711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.204365</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>1.216329</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.197113</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>1.209582</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.189930</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>1.202930</td>\n",
       "      <td>0.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.182874</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>1.196401</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.175867</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>1.189908</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.168991</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>1.183558</td>\n",
       "      <td>0.000852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.162197</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>1.177286</td>\n",
       "      <td>0.000860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.155509</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>1.171105</td>\n",
       "      <td>0.000875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.148927</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>1.165031</td>\n",
       "      <td>0.000890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.142384</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>1.159005</td>\n",
       "      <td>0.000898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>0.320617</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.593623</td>\n",
       "      <td>0.004098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>0.320562</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.593622</td>\n",
       "      <td>0.004096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>0.320507</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.593622</td>\n",
       "      <td>0.004098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>0.320462</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.593623</td>\n",
       "      <td>0.004098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>0.320419</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.593621</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>0.320366</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.593620</td>\n",
       "      <td>0.004096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>0.320301</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.593616</td>\n",
       "      <td>0.004097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>0.320252</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.593612</td>\n",
       "      <td>0.004096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>0.320197</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.593605</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>0.320146</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.593606</td>\n",
       "      <td>0.004096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>0.320089</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.593602</td>\n",
       "      <td>0.004096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>0.320031</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.593598</td>\n",
       "      <td>0.004098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>0.319971</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.593594</td>\n",
       "      <td>0.004099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>0.319912</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.593593</td>\n",
       "      <td>0.004103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>0.319856</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.593587</td>\n",
       "      <td>0.004101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>0.319806</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.593588</td>\n",
       "      <td>0.004103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>0.319754</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.593593</td>\n",
       "      <td>0.004106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2313</th>\n",
       "      <td>0.319693</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.593589</td>\n",
       "      <td>0.004109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2314</th>\n",
       "      <td>0.319647</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.593590</td>\n",
       "      <td>0.004106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2315</th>\n",
       "      <td>0.319602</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.593588</td>\n",
       "      <td>0.004106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>0.319547</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.004108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317</th>\n",
       "      <td>0.319487</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.593588</td>\n",
       "      <td>0.004106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.593591</td>\n",
       "      <td>0.004109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>0.319390</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.593593</td>\n",
       "      <td>0.004113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>0.319334</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.593591</td>\n",
       "      <td>0.004114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>0.319281</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.593591</td>\n",
       "      <td>0.004118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>0.319228</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.593591</td>\n",
       "      <td>0.004119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>0.319176</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.593589</td>\n",
       "      <td>0.004117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>0.319110</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.593583</td>\n",
       "      <td>0.004120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>0.319064</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.593582</td>\n",
       "      <td>0.004126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2326 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      train-mlogloss-mean  train-mlogloss-std  test-mlogloss-mean  \\\n",
       "0                1.376122            0.000031            1.376718   \n",
       "1                1.366150            0.000068            1.367342   \n",
       "2                1.356351            0.000040            1.358133   \n",
       "3                1.346763            0.000049            1.349125   \n",
       "4                1.337293            0.000081            1.340250   \n",
       "5                1.327997            0.000108            1.331541   \n",
       "6                1.318822            0.000143            1.322930   \n",
       "7                1.309828            0.000149            1.314488   \n",
       "8                1.300931            0.000169            1.306182   \n",
       "9                1.292124            0.000207            1.297967   \n",
       "10               1.283534            0.000280            1.289945   \n",
       "11               1.275059            0.000268            1.282037   \n",
       "12               1.266687            0.000300            1.274229   \n",
       "13               1.258490            0.000297            1.266578   \n",
       "14               1.250427            0.000323            1.259072   \n",
       "15               1.242452            0.000306            1.251667   \n",
       "16               1.234642            0.000284            1.244387   \n",
       "17               1.226906            0.000327            1.237205   \n",
       "18               1.219303            0.000310            1.230154   \n",
       "19               1.211777            0.000319            1.223186   \n",
       "20               1.204365            0.000334            1.216329   \n",
       "21               1.197113            0.000388            1.209582   \n",
       "22               1.189930            0.000428            1.202930   \n",
       "23               1.182874            0.000460            1.196401   \n",
       "24               1.175867            0.000451            1.189908   \n",
       "25               1.168991            0.000461            1.183558   \n",
       "26               1.162197            0.000491            1.177286   \n",
       "27               1.155509            0.000510            1.171105   \n",
       "28               1.148927            0.000522            1.165031   \n",
       "29               1.142384            0.000576            1.159005   \n",
       "...                   ...                 ...                 ...   \n",
       "2296             0.320617            0.000882            0.593623   \n",
       "2297             0.320562            0.000882            0.593622   \n",
       "2298             0.320507            0.000878            0.593622   \n",
       "2299             0.320462            0.000883            0.593623   \n",
       "2300             0.320419            0.000878            0.593621   \n",
       "2301             0.320366            0.000874            0.593620   \n",
       "2302             0.320301            0.000880            0.593616   \n",
       "2303             0.320252            0.000881            0.593612   \n",
       "2304             0.320197            0.000876            0.593605   \n",
       "2305             0.320146            0.000875            0.593606   \n",
       "2306             0.320089            0.000870            0.593602   \n",
       "2307             0.320031            0.000864            0.593598   \n",
       "2308             0.319971            0.000867            0.593594   \n",
       "2309             0.319912            0.000866            0.593593   \n",
       "2310             0.319856            0.000873            0.593587   \n",
       "2311             0.319806            0.000870            0.593588   \n",
       "2312             0.319754            0.000873            0.593593   \n",
       "2313             0.319693            0.000864            0.593589   \n",
       "2314             0.319647            0.000861            0.593590   \n",
       "2315             0.319602            0.000859            0.593588   \n",
       "2316             0.319547            0.000855            0.593592   \n",
       "2317             0.319487            0.000845            0.593588   \n",
       "2318             0.319444            0.000845            0.593591   \n",
       "2319             0.319390            0.000849            0.593593   \n",
       "2320             0.319334            0.000853            0.593591   \n",
       "2321             0.319281            0.000848            0.593591   \n",
       "2322             0.319228            0.000846            0.593591   \n",
       "2323             0.319176            0.000845            0.593589   \n",
       "2324             0.319110            0.000841            0.593583   \n",
       "2325             0.319064            0.000838            0.593582   \n",
       "\n",
       "      test-mlogloss-std  \n",
       "0              0.000056  \n",
       "1              0.000114  \n",
       "2              0.000162  \n",
       "3              0.000184  \n",
       "4              0.000196  \n",
       "5              0.000231  \n",
       "6              0.000285  \n",
       "7              0.000309  \n",
       "8              0.000338  \n",
       "9              0.000347  \n",
       "10             0.000354  \n",
       "11             0.000369  \n",
       "12             0.000401  \n",
       "13             0.000460  \n",
       "14             0.000499  \n",
       "15             0.000552  \n",
       "16             0.000603  \n",
       "17             0.000636  \n",
       "18             0.000685  \n",
       "19             0.000711  \n",
       "20             0.000741  \n",
       "21             0.000745  \n",
       "22             0.000793  \n",
       "23             0.000804  \n",
       "24             0.000847  \n",
       "25             0.000852  \n",
       "26             0.000860  \n",
       "27             0.000875  \n",
       "28             0.000890  \n",
       "29             0.000898  \n",
       "...                 ...  \n",
       "2296           0.004098  \n",
       "2297           0.004096  \n",
       "2298           0.004098  \n",
       "2299           0.004098  \n",
       "2300           0.004100  \n",
       "2301           0.004096  \n",
       "2302           0.004097  \n",
       "2303           0.004096  \n",
       "2304           0.004100  \n",
       "2305           0.004096  \n",
       "2306           0.004096  \n",
       "2307           0.004098  \n",
       "2308           0.004099  \n",
       "2309           0.004103  \n",
       "2310           0.004101  \n",
       "2311           0.004103  \n",
       "2312           0.004106  \n",
       "2313           0.004109  \n",
       "2314           0.004106  \n",
       "2315           0.004106  \n",
       "2316           0.004108  \n",
       "2317           0.004106  \n",
       "2318           0.004109  \n",
       "2319           0.004113  \n",
       "2320           0.004114  \n",
       "2321           0.004118  \n",
       "2322           0.004119  \n",
       "2323           0.004117  \n",
       "2324           0.004120  \n",
       "2325           0.004126  \n",
       "\n",
       "[2326 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 0.01 / 2326\n",
    "cvresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* best paramter test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### xgb\n",
    "grid_result = []\n",
    "param = {}\n",
    "#### XGB parameters\n",
    "## General Parameters\n",
    "param['n_gpus'] = -1\n",
    "param['tree_method'] = 'gpu_hist'\n",
    "param['silent'] = 0\n",
    "\n",
    "## Booster Parameters\n",
    "param['n_estimators'] = 369 #요기...\n",
    "param['learning_rate'] = 0.1\n",
    "param['min_child_weight'] = 2\n",
    "param['max_depth'] = 10\n",
    "param['gamma'] = 0\n",
    "param['reg_alpha'] = 0.01\n",
    "param['reg_lambda'] = 0.05\n",
    "param['subsample'] = 0.95\n",
    "param['colsample_bytree'] = 0.75\n",
    "param['scale_pos_weight'] = 1\n",
    "\n",
    "## Learning task parameters\n",
    "param['num_class'] = 4\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['seed'] = 7\n",
    "\n",
    "cv_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.75, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=2, missing=None, n_estimators=369,\n",
       "       n_gpus=-1, n_jobs=1, nthread=None, num_class=4,\n",
       "       objective='multi:softmax', random_state=0, reg_alpha=0.01,\n",
       "       reg_lambda=0.05, scale_pos_weight=1, seed=7, silent=0,\n",
       "       subsample=0.95, tree_method='gpu_hist'),\n",
       "       fit_params=None, iid=False, n_jobs=1,\n",
       "       param_grid={'reg_alpha': [0.01], 'reg_lambda': [0.05]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_macro', verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 그리드....\n",
    "print()\n",
    "print(\"===============================================\")\n",
    "param_tune_reg = {}\n",
    "param_tune_reg['reg_alpha'] = [0.01]\n",
    "param_tune_reg['reg_lambda'] = [0.05]\n",
    "\n",
    "grid = GridSearchCV(estimator = XGBClassifier(**param),param_grid = param_tune_reg, scoring = 'f1_macro',iid=False ,n_jobs = 1, cv = cv_folds)\n",
    "grid.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the Regularization alpha = L1, lambda = L2\n",
      "Grid Scores\n",
      "[mean: 0.73676, std: 0.00386, params: {'reg_alpha': 0.01, 'reg_lambda': 0.05}]\n",
      "Best parameter - Regularization alpha = L1, lambda = L2\n",
      "{'reg_alpha': 0.01, 'reg_lambda': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Find the Regularization alpha = L1, lambda = L2\")\n",
    "print(\"Grid Scores\")\n",
    "pprint(grid.grid_scores_)\n",
    "grid_result.append(grid.grid_scores_)\n",
    "print(\"Best parameter - Regularization alpha = L1, lambda = L2\")\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/9f/f6324af3fc43f352e568b5850695c30ed7dd14af06a94f97953ff9187569/hyperopt-0.1.1-py3-none-any.whl (117kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from hyperopt) (1.13.3)\n",
      "Collecting future (from hyperopt)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/2b/8d082ddfed935f3608cc61140df6dcbf0edea1bc3ab52fb6c29ae3e81e85/future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 3.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pymongo (from hyperopt)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/88/dd1f8c4281a60791b043f55e338d0521049208f21e3de19ddc9c160dbbef/pymongo-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (405kB)\n",
      "\u001b[K    100% |████████████████████████████████| 409kB 5.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from hyperopt) (1.1.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.6/site-packages (from hyperopt) (2.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from hyperopt) (1.11.0)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /opt/conda/lib/python3.6/site-packages (from networkx->hyperopt) (4.3.0)\n",
      "Building wheels for collected packages: future\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/bf/c9/a3/c538d90ef17cf7823fa51fc701a7a7a910a80f6a405bf15b1a\n",
      "Successfully built future\n",
      "Installing collected packages: future, pymongo, hyperopt\n",
      "Successfully installed future-0.16.0 hyperopt-0.1.1 pymongo-3.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
